{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOopNAu2tZpHWP1oVisC63E"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## Issues with Feed Forward Network\n","- ANN & CNN is not capable of handling sequential or time-series data (sentences, stock price, video stream, etc...)\n","\n","- It only considers the current input, does not memorize the previous inputs.\n","\n","- Assumes that the data points are independent of each other. Time-series is not like that. Even in sentences, its important to remember the previous context to understand the meaning.\n","\n","**RNN** and **LSTM** is the solution to this problem."],"metadata":{"id":"Wv5rXjDP1Ur9"}},{"cell_type":"markdown","source":["## RNN\n","It analyzes the data by finding the sequence of frequenctly occuring words.\n","\n","It is specialized ANN designed for sequence data:\n","- Time series (current value is influenced by previous data)\n","- Natutal language\n","- Speech\n","- Music\n","\n","Lets says, you have 10 days of data that is fed to RNN as 10 inputs.\n","On pass1, it processes input1\n","On Pass2, it processes input2 + pass1 output\n","On Pass3, it processes input3 + pass2 & 3 output\n","and so on....\n","By doing so, it remembers the previous context.\n","\n","### Challenges\n","- Vanishing gradients because of too many layers. if the weight in fraction (0.3), when the input is multiplied with the weight too many times, the meaning of input vanishes.\n","- On every unrolling, the input is mutliplied with weight. It result in high value. It is called gradient exploding. the meaning of input is lost\n","- This is the reason, RNN does not remember long-term dependencies."],"metadata":{"id":"VL2ZJG4c2nEz"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"TWARNVy01OoP"},"outputs":[],"source":[]},{"cell_type":"markdown","source":["### LSTM\n","LSTM is a special kind of RNN designed to lean long-term dependencies.\n","It solves the vanishing gradient problem that makes basic RNNs struggle with the long sequences.\n","\n","Applications\n","- Sentiment Analysis\n","- Speech recognition\n","- time series\n","- text generation\n","- chatbot"],"metadata":{"id":"Xm1bJ2_9_839"}}]}