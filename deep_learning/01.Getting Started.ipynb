{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOBSnA8aqTrjgSpp5/u9y45"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# What is Deep Learning\n","It is a subset of machine learning that takes data, perform a function, and progressively gets better at it. The algorithms are inspired by the structure and function of the brain called **Artificial Nural Network (ANN)**.\n","\n","## Agenda - 7/25\n","- Neural Network and Deep Learning Foundation\n","- Tunning and Optimizing the Deep Learning Networks\n","- Convolution Neural Networks (CNN) - I --> CNN deals with images\n","- Convolution Neural Networks - II\n","- Recurrent Neutal Networks (RNN) --> deals with time-series data\n","- Long Short-Term Memory (LSTM) Networks --> deals with time-series data\n","\n","**Use-Cases**\n","- Face-detecting features in the Facebook or iPhone.\n","- Object-detection by the autonomous cars.\n","- Speech-recognition with Siri\n","- Recommending movies to users - Netflix\n","\n","https://poloclub.github.io/cnn-explainer/\n","\n"],"metadata":{"id":"PYxbuFxymIec"}},{"cell_type":"markdown","source":["# Machine Learning vs Deep Learning\n","\n","![DLvsML](https://cdn.prod.website-files.com/634e9d5d7cb8f7fc56b28199/66a2f1efb5b625f74538bd2a_668f001243b877277fd25aa1_652ebc26fbd9a45bcec81819_Deep_Learning_vs_Machine_Learning_3033723be2.jpeg)\n","\n","\n","  - Machine learning is trained on small data compared with Deep Learning.\n","  - ML works on algorithms. DL works on ANN.\n","  - Training time is short for ML. It is long for ANN.\n","  - Accuracy of the DL increases with more training data. But it drops for ML."],"metadata":{"id":"2hMl4j_zrOBw"}},{"cell_type":"markdown","source":["## Perceptrons\n","\n","The Perceptron is one of the simplest types of artificial neural networks and forms the foundation of more complex deep learning models. It is a **binary classifier** that **maps input features to a single output using a set of weights and a bias**, followed by **an activation function**.\n","\n","![DLvsML](https://miro.medium.com/v2/resize:fit:1100/format:webp/0*Ib3_FfuOy04kOmfO)\n","\n","The following things take place in each Perceptron in the ANN:\n","- It receives vector inputs and its weights\n","- It performs weighted sum = x1w1 + x2w2 + ....+XnWn\n","- the weighted sum is sent to activation function f(x) to generate output.\n","\n","**Activation Function** introduces non-linearity into the network, which allows it to learn complex patterns (like images, language, etc.). Without activation functions, a neural network would just be a big linear function, no matter how many layers it has.\n","\n","### Problem with Linearity\n","Doctors want to predict if a tumor is cancerous based on things like shape, size, and texture.\n","score= a‚ãÖsize + b‚ãÖtexture + c‚ãÖshape+d\n","\n","If score is high ‚Üí cancer and If score is low ‚Üí not cancer\n","\n","This **assumes everything affects the result in a straight line.**\n","\n","Example: Every 1 unit increase in texture always increases cancer risk the same amount.\n","**This is too simple for the real world.**\n","\n","Example:\n","\n","- A large cell might only mean cancer if it‚Äôs also irregular.\n","\n","- A rough texture might be OK unless the shape is also weird.\n","\n","Hence, some non-lineraity ingestion by the **activation function** is needed.\n","\n","**ANN** is a collection of layers of **Perceptron** connected. It consists of:\n","- Input Layer\n","- Hidden Layers\n","- Output Layer\n","\n","E.g an image is visualize as an array of x * y dimension of pixels. The array is sent to each neuron/perceptron in the input layer. Out put of the layer is sent to each neuron/perceptron in the next hidden layer, and so on.\n","\n","https://playground.tensorflow.org/#\n","\n","https://developers.google.com/machine-learning/crash-course/neural-networks/activation-functions"],"metadata":{"id":"ZxxM3sbivOlq"}},{"cell_type":"markdown","source":["# Input to Activation Function or Transfer Function in Artificial Neural Networks (ANN)\n","\n","In an **Artificial Neural Network (ANN)**, the **input to the activation function** is also known as:\n","\n","- **Net input**\n","- **Weighted sum**\n","- **Pre-activation value**\n","\n","As mentioned above, the Activation function adds non-linearity to the input to learn complex underlying patterns in the data. **without activation function**, the neuron resembles a linear regression.  \n","\n","---\n","\n","### üî¢ Formula\n","\n","For a neuron/perceptron $j $, the input to the activation function is:\n","\n","$$\n","z_j = \\sum_{i=1}^{n} w_{ji} x_i + b_j\n","$$\n","\n","Where:\n","- $ x_i $ = input from neuron $ i $ (or input feature)\n","- $ w_{ji} $ = weight connecting input $ i $ to neuron $ j $\n","- $ b_j $ = bias of neuron $ j $\n","- $ z_j $ = net input to the activation function\n","\n","The output of the neuron is then:\n","\n","$$\n","a_j = f(z_j)\n","$$\n","\n","Where $ f $ is the **activation function** (like ReLU, Sigmoid, or Tanh), and $ a_j $ is the output.\n","\n","**üß† Example**\n","\n","Given:\n","- Inputs: $ x_1 = 0.5 $, $ x_2 = 0.8 $\n","- Weights: $ w_1 = 0.3 $, $ w_2 = 0.7 $\n","- Bias: $ b = 0.1 $\n","\n","Calculate the net input:\n","\n","$$\n","z = (0.5 \\times 0.3) + (0.8 \\times 0.7) + 0.1 = 0.15 + 0.56 + 0.1 = 0.81\n","$$\n","\n","---\n","\n","Here are some of the **activation functions**:\n","\n","### 1. Step Function (Heaviside)\n","It is not used anymore.\n","\n","$$\n","f(x) = \\begin{cases} 1 & \\text{if } x > 0 \\\\ 0 & \\text{otherwise} \\end{cases}\n","$$\n","\n","**Graph**\n","A sudden jump from 0 to 1 at ùë• = 0\n","\n","\n","### 2. ReLU Function\n","\n","If we use ReLU as the activation function:\n","\n","$$\n","a = \\max(0, 0.81) = 0.81\n","$$\n","\n","**Graph**\n","- 0 for all negative values\n","- Straight line for positive values\n","- Looks simple, but creates a kink at 0\n","- This kink breaks the linearity\n","\n","**Usage**\n","- **Almost every modern neural network (CNNs, MLPs, Transformers)**\n","- It is used only in the Hdden layer.\n","\n","### 3. Sigmoid Function\n","\n","$$\n","\\sigma(z) = \\frac{1}{1 + e^{-z}}\n","$$\n","\n","Where:\n","- $ \\sigma(z) $ is the output of the sigmoid function\n","- $ z $ is the input (also called the net input or weighted sum)\n","- $ e $ is Euler's number, approximately equal to 2.718\n","\n","**Graph**\n","- S-shaped curve\n","- Ranges from 0 to 1\n","- Smoothly squashes input into range (0 to 1)\n","- Allows small and large values to flatten out\n","- Non-linear because the rate of change is not constant\n","\n","**Usage**\n","- **Binary classification** outputs\n","- Hidden layers of RNN\n","- Used mostly in output layer.\n","\n","### 4. TanH Function\n","\n","$$\n","\\tanh(z) = \\frac{e^{z} - e^{-z}}{e^{z} + e^{-z}}\n","$$\n","\n","Where:  \n","- \\( \\tanh(z) \\) is the output of the hyperbolic tangent function  \n","- \\( z \\) is the input (also called the net input or weighted sum)  \n","- \\( e \\) is Euler's number, approximately equal to 2.718  \n","\n","**Graph**  \n","- S-shaped curve, similar to sigmoid but centered at zero  \n","- Ranges from \\(-1\\) to \\(1\\)  \n","- Smoothly squashes input into range \\(-1\\) to \\(1\\)  \n","- Allows both positive and negative inputs to be transformed non-linearly  \n","- Non-linear because the rate of change varies depending on input value  \n","\n","**Usage**  \n","- Often used in hidden layers of neural networks  \n","- Especially popular in **recurrent neural networks** (RNNs)  \n","- Helps center data around zero which can improve learning speed and stability  \n","- Hidden layers of RNN\n","\n","### 5. Softmax Function\n","$$\n","\\text{softmax}(z_i) = \\frac{e^{z_i}}{\\sum_{j=1}^{K} e^{z_j}}\n","$$\n","\n","Where:  \n","- $ z_i $ is the input to the $ i^{\\text{th}} $ neuron in the output layer  \n","- $ K $ is the total number of output classes  \n","- $ e $ is Euler‚Äôs number, approximately equal to 2.718  \n","- The denominator sums over all $ K $ class scores, turning raw scores into probabilities  \n","\n","**Graph**  \n","- Converts raw input values (called logits) into a **probability distribution**  \n","- All outputs are between \\( 0 \\) and \\( 1 \\)  \n","- The sum of all outputs is exactly \\( 1 \\)  \n","- Emphasizes the **largest values** while suppressing smaller ones  \n","- Non-linear because the exponential and normalization change the scale and shape of input values  \n","\n","**Usage**  \n","- Used in the **output layer** of neural networks for **multi-class classification**  \n","- Helps the network output probabilities for each class  \n","- Most common in tasks like image classification, text categorization, etc.  \n","- Not used in hidden layers  \n","\n","\n","### ‚úÖ Notes\n","\n","- **Input Layer** is meant for receiving inputs only. It does not use any activation function.\n","\n","\n","\n","\n"],"metadata":{"id":"qUUG_Mf51giq"}},{"cell_type":"markdown","source":["# üîÅ Training Nural Network - Forward Propagation vs. Backward Propagation\n","\n","https://xnought.github.io/backprop-explainer/\n","\n","---\n","\n","### üöÄ Forward Propagation\n","\n","**Definition**:  \n","Forward propagation is the process of **passing input data through the network** to generate an output (prediction).\n","\n","#### üî¢ Steps:\n","1. **Input** data is fed to the input layer.\n","2. Each neuron computes a **weighted sum** of its inputs:\n","   $$\n","   z = \\sum w_i x_i + b\n","   $$\n","3. The result is passed through an **activation function** (e.g., ReLU, Sigmoid):\n","   $$\n","   a = f(z)\n","   $$\n","4. This continues layer by layer until the **final output** is produced.\n","\n","#### üéØ Purpose:\n","To calculate the **output (prediction)** of the neural network based on current weights.\n","\n","$$\n","   \\text{Loss} = \\text{LossFunction}(y_{\\text{true}}, y_{\\text{pred}})\n","   $$\n","\n","Objective of learning is to minimize the losses. **In forward-propagation**, *there is no way to send feedback back to the model training.*\n","\n","---\n","\n","### üîÅ Backward Propagation (Backprop)\n","\n","**Definition**:  \n","Backward propagation is the process of **updating the weights** by calculating the gradient of the loss function and applying optimization.\n","\n","#### üîÑ Steps:\n","1. Calculate the **loss (error)** between predicted output and true label:\n","   $$\n","   \\text{Loss} = \\text{LossFunction}(y_{\\text{true}}, y_{\\text{pred}})\n","   $$\n","2. Compute the **gradient of the loss** with respect to each weight using the **chain rule** of calculus.\n","3. Propagate the error **backward from output to input** layers.\n","4. **Update the weights** using Gradient Descent:\n","   $$\n","   w_{\\text{new}} = w_{\\text{old}} - \\eta \\cdot \\frac{\\partial \\text{Loss}}{\\partial w_{\\text{old}}}\n","   $$\n","   Where $ \\eta $ is the learning rate.\n","\n","#### üéØ Purpose:\n","To **minimize the loss** by adjusting weights and biases in the direction that reduces error.\n","\n","---\n","\n","### üß™ Simple Analogy\n","\n","- **Forward Propagation**: Like a student answering a test based on what they know.\n","- **Backward Propagation**: Like the teacher grading the test, showing mistakes, and the student learning from it to do better next time.\n","\n","---\n","\n","### üîÑ Summary Table\n","\n","| Process              | Direction           | Purpose                       | Main Operation            |\n","|----------------------|---------------------|-------------------------------|----------------------------|\n","| Forward Propagation  | Input ‚Üí Output       | Make predictions               | Weighted sum + activation |\n","| Backward Propagation | Output ‚Üí Input       | Update weights (learn)         | Gradient computation + update |\n","\n","\n","\n","**EPOCH**: A forward and backward propagation completes 1 EPOCH.\n","\n","EPOCH count tells you how many forward-backward propagation took place in the learning.\n","\n","\n"],"metadata":{"id":"Ki3pW7QWF6Fh"}},{"cell_type":"markdown","source":["# üß† Predicting an Image Using a Neural Network (Backward Propagation)\n","\n","Let's walk through how a neural network predicts whether an image is a **cat or dog**, using key concepts like **initial weights**, **activation functions**, **forward pass**, and **backpropagation**.\n","\n","---\n","\n","## üî¢ Step 1: Input Layer\n","\n","- A 28√ó28 grayscale image is flattened into a vector of pixel values.\n","- For a 28√ó28 image ‚Üí 784 inputs:\n","\n","$$\n","x = [x_1, x_2, ..., x_{784}]\n","$$\n","\n","üëâ **No activation function is applied** at this stage. It's just raw input.\n","\n","---\n","\n","## üîß Step 2: Initialize Weights\n","\n","- Every connection from the input to the first hidden layer gets an initial weight.\n","- Example using **He Initialization** (good for ReLU):\n","\n","$$\n","w \\sim \\mathcal{N}(0, \\frac{2}{n_{\\text{in}}})\n","$$\n","\n","This helps avoid exploding or vanishing gradients.\n","\n","---\n","\n","## üßÆ Step 3: Forward Pass\n","\n","### Hidden Layer\n","\n","Let‚Äôs say we have:\n","\n","- 1 Hidden Layer with 128 neurons (ReLU activation)\n","- 1 Output Neuron (Sigmoid activation for binary classification)\n","\n","The weighted sum for one hidden neuron is:\n","\n","$$\n","z = w_1x_1 + w_2x_2 + \\dots + w_{784}x_{784} + b\n","$$\n","\n","Apply **ReLU** activation:\n","\n","$$\n","a = \\max(0, z)\n","$$\n","\n","Repeat for all 128 neurons.\n","\n","### Output Layer\n","\n","Then:\n","\n","$$\n","z_{\\text{output}} = \\sum w_i a_i + b_{\\text{out}}\n","$$\n","\n","$$\n","\\hat{y} = \\sigma(z_{\\text{output}}) = \\frac{1}{1 + e^{-z_{\\text{output}}}}\n","$$\n","\n","- If $ \\hat{y} \\approx 1 $: predicted label is **dog**\n","- If $ \\hat{y} \\approx 0 $: predicted label is **cat**\n","\n","---\n","\n","## üí• Step 4: Loss Calculation\n","\n","Compare the prediction to the actual label using **binary cross-entropy**:\n","\n","$$\n","\\text{Loss} = -\\left[y \\cdot \\log(\\hat{y}) + (1 - y) \\cdot \\log(1 - \\hat{y})\\right]\n","$$\n","\n","Where:\n","- $ y $ is the true label (1 for dog, 0 for cat)\n","- $ \\hat{y} $ is the predicted probability\n","\n","---\n","\n","## üîÑ Step 5: Backpropagation\n","\n","- Compute the **error** at the output layer.\n","- Use the **chain rule** to propagate the error backward.\n","- Compute gradients:\n","\n","$$\n","\\frac{\\partial \\text{Loss}}{\\partial w}\n","$$\n","\n","- Update weights using **gradient descent**:\n","\n","$$\n","w = w - \\eta \\cdot \\frac{\\partial \\text{Loss}}{\\partial w}\n","$$\n","\n","Where $ \\eta $ is the **learning rate**.\n","\n","---\n","\n","## üîÅ Repeat for Many Images\n","\n","This process repeats for **every image**, over many **epochs**, improving accuracy each time.\n","\n","The process repeats until stopping conditions is met. Here are some stopping condition set by you.\n","\n","1. Number of Epoch\n","\n","2. Stop training if loss < 0.001 or if it stops improving for 5 epochs\n","\n","3. If validation loss starts increasing, it means the model is overfitting.\n","\n","4. Maximum training time\n","\n","---\n","\n","## üß© Summary Flowchart\n","\n","Image (28x28) ‚Üí Flatten ‚Üí Input Layer\n","\n","         ‚Üì\n","\n","     Hidden Layer 1 (ReLU)\n","\n","         ‚Üì\n","\n","     Output Layer (Sigmoid)\n","\n","         ‚Üì\n","\n","   Prediction: \"Dog\" or \"Cat\"\n","\n","         ‚Üì\n","\n","    Compute Loss & Backprop\n","\n","         ‚Üì\n","\n","     Update Weights\n","\n","\n"],"metadata":{"id":"AuEO7grcG8Pa"}},{"cell_type":"markdown","source":["## Loss Functions\n","- **Mean Squared Error (MSE)**: for regression tasks  \n","  $$\n","  MSE = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n","  $$\n","\n","- **Binary Cross-Entropy**: for binary classification  \n","  $$\n","  BCE = - \\frac{1}{n} \\sum_{i=1}^{n} \\left( y_i \\cdot \\log \\hat{y}_i + (1 - y_i) \\cdot \\log (1 - \\hat{y}_i) \\right)\n","  $$\n","\n","- **Categorical Cross-Entropy**: for multi-class classification  \n","  $$\n","  Loss = - \\sum_{i=1}^{output \\ size} y_i \\cdot \\log \\hat{y}_i\n","  $$\n","\n","Where:  \n","- **n**: Number of training samples;  \n","- **y·µ¢**: Actual Output;  \n","- **≈∑·µ¢**: Predicted class probabilities.\n"],"metadata":{"id":"dizgxBL3wz0x"}},{"cell_type":"markdown","source":["**Batch Size**\n","No of training samples processed together in one training step.\n","\n","e.g. let's say I have 1000 images dataset. It is processed in batches. say I want to processes 100 images per batch.\n","\n","In that case, No of batches = 10, Size of Each Batch or Batch Size = 10\n","\n","**EPOCH**\n","One full pass through the entire dataset is called EPOCH, composed of multiple itertations. E.g processing all batches composing 1000 images.\n","\n","**Learning Rate** A hyper-parameter that detemines the model weight are adjusted during each update step.\n","It tells how frequently the weights are adjusted."],"metadata":{"id":"9Vd43B-XxUi2"}}]}