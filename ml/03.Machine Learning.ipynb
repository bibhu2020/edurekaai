{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyM9hnmqroj5NzV7eWS6bEfP"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Machine Learning"],"metadata":{"id":"mVrVU9R3MCMT"}},{"cell_type":"markdown","source":["## 1. Supervised Learning\n","- Input data (input features + output) is labelled. Models are trained using labelled dataset, where the model lears about each type of data.\n","\n","- Once the training process is completed, the model is tested on the basis of test data (a subset of training set), and then it predicts the output.\n","\n","\n","<img src=\"https://cdn.prod.website-files.com/5ef788f07804fb7d78a4127a/61cc3ae3acdd638008a38147_supervised%20learning.png\">\n","\n","Supervised learning is devided into 2 types:\n","- **Regression**\n","- **Classification**"],"metadata":{"id":"Uy7GouBGML-3"}},{"cell_type":"markdown","source":["### 1.1 Regression\n","It is used when the **dependent variable** is a continious numerical value.\n","\n","E.g. Predicting tomorrow's temperature.\n","\n","Regression Algorithms are:\n","- Linear Regression\n","- Decission Tree\n","- Random Forest\n","\n","#### 1.1.1 Linear Regression\n","y = m * x + c\n","\n","Input features\n","- x\n","\n","Output feature\n","- y\n","\n","Machine learning objective here is to find the value of m (slope) and c (y-intercepter) so that we can apply input (x) and predice the output (y).\n","\n","------------------------------\n","Mulitple-Linear Regression looks something like this:\n","$$\n","y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\dots + \\beta_p x_p + \\epsilon\n","$$\n","\n","#### **Residual Error**\n","In the machine learning context, the Residual Error = (Y-actual) - (Y-predicted). The machine learning may predict multiple linear-line or model. You will have to choose the one based on Error or Accuracy Metrics.\n","\n","**Error Metrics** measure how far off your predictions are from the actual values. Lower values mean better performance.\n","\n","**_Mean Absolute Errors (MAE)_**\n","$$\n","MAE = \\frac{1}{n} \\sum_{i=1}^{n} \\left| y_i - \\hat{y}_i \\right|\n","$$\n","\n","**_Mean Squared Errors (MSE)_**\n","$$\n","MSE = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n","$$\n","\n","**_Root Mean Square Error (RMSE)_**\n","$$\n","RMSE = \\sqrt{ \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 }\n","$$\n","\n","**_Sum of Squared Errors (SSE)_**\n","$$\n","SSE = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n","$$\n","\n","\n","**Accuracy Metrics**\n","tell you how well your model is performing overall — in other words, how close the predictions are to the true values, either in percentage or in explained variance. Here are a few:\n","\n","**_R² - Coefficient of Determination_**\n","$$\n","R^2 = 1 - \\frac{SSE}{SST} = 1 - \\frac{\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2}{\\sum_{i=1}^{n}(y_i - \\bar{y})^2}\n","$$\n","\n","**_Regression Accuracy_**\n","$$\n","MAPE = \\frac{100\\%}{n} \\sum_{i=1}^{n} \\left| \\frac{y_i - \\hat{y}_i}{y_i} \\right|\n","$$\n","\n","$$\n","\\text{Accuracy} = 100\\% - \\text{MAPE}\n","$$\n","\n","\n","#### **📊 R² (R-squared) Interpretation**\n","\n","| R² Value | Interpretation                                      |\n","|----------|-----------------------------------------------------|\n","| 0        | Model explains **none** of the variance             |\n","| 1        | Model explains **all** the variance (perfect fit)   |\n","| ~0.7+    | Generally considered **good** in most domains       |\n","| < 0.5    | **Weak** model — may need improvement or tuning     |\n","\n","#### **📊 RMSE Interpretation Table (Rule of Thumb)**\n","\n","| Scenario                                             | RMSE Interpretation                         |\n","|------------------------------------------------------|---------------------------------------------|\n","| RMSE ≈ 0                                             | ✅ Perfect predictions                       |\n","| RMSE ≪ mean(target) or std(target)                   | ✅ Very good model                           |\n","| RMSE ≈ std(target)                                   | ⚖️ Acceptable model                          |\n","| RMSE ≫\n"],"metadata":{"id":"eIYA_D8mPliT"}},{"cell_type":"markdown","source":["### 1.2 Classification\n","\n","It is used when the **Dependent variable** is a categorical finite set of value.\n","\n","E.g. Predict tomorrow's weather to be hot / clody / rain.\n","\n","Classification Algorithms are:\n","- Logistic Regression\n","- Decission Tree\n","- Random Forest\n","- K Nearest Neighbor\n","- SVM - Support Vector Machine\n","- Ensemble Learning\n","- Naive Bayes\n","- etc..\n","\n","#### 1.2.1 Logistic Regression\n","Logistic Regression is a supervised machine learning algorithm used for **classification problems** — when your target variable is categorical (e.g., Yes/No, 0/1, Spam/Not Spam).\n","\n","Despite the name, it is not used for regression, but for predicting probabilities and making binary or multi-class decisions.\n","\n","Logistic Regression predicts the probability that a given input belongs to a particular class using the sigmoid (logistic) function.\n","\n","##### 📊 Logistic Regression Probability Function\n","\n","The logistic regression model predicts the **probability** that the output `y` is 1 (e.g., \"Yes\", \"Approved\", \"Positive\") given the input features `x`.\n","\n","$$\n","P(y = 1 \\mid x) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\cdots + \\beta_p x_p)}}\n","$$\n","\n","##### 🧠 Explanation:\n","\n","- $P(y = 1 \\mid x)$: Probability that the target `y` equals 1 given input features `x`\n","- $\\beta_0 $: Intercept (bias term)\n","- $\\beta_1, \\beta_2, \\ldots, \\beta_p $: Coefficients (weights) for each feature\n","- $ x_1, x_2, \\ldots, x_p $: Feature values\n","- $ e $: Euler’s number (approx. 2.718)\n","\n","##### 📊 Multinomial Logistic Regression\n","\n","**Multinomial Logistic Regression** is an extension of binary logistic regression used when the **target variable has more than two classes** (e.g., `Accepted`, `Rejected`, `Withdrawn`).\n","\n","Unlike One-vs-Rest, it models **all classes simultaneously** using the **softmax function**.\n","\n","---\n","\n","##### 🧠 Softmax Function\n","\n","Given a feature vector \\( x \\), the probability that it belongs to class \\( k \\) is:\n","\n","$$\n","P(y = k \\mid x) = \\frac{e^{z_k}}{\\sum_{j=1}^{K} e^{z_j}}\n","$$\n","\n","Where:\n","- $ z_k = \\beta_{k0} + \\beta_{k1}x_1 + \\cdots + \\beta_{kp}x_p $\n","- $ K $ is the total number of classes\n","- $ \\beta_{ki} $ are the learned coefficients for class \\( k \\)\n","\n","---\n","\n","##### 🔍 How It Works\n","\n","1. Compute a **score** \\( z_k \\) for each class using a linear combination of inputs.\n","2. Apply the **softmax** to convert these scores into **probabilities** that sum to 1.\n","3. Choose the class with the **highest probability** as the predicted class.\n","\n","\n","\n","It's called **logistic regression** because it models the probability of a class using a regression-like linear equation (shown below) that is then passed through the logistic (sigmoid) function — not because it performs regression in the traditional sense of predicting continuous values.\n","\n","$ z_k = \\beta_{k0} + \\beta_{k1}x_1 + \\cdots + \\beta_{kp}x_p $\n","\n","**Note**:\n","\n","Tree-based Algorithms and Decission Tree are on its own ipynb file.\n","\n"],"metadata":{"id":"u7sQ2tipPpz5"}},{"cell_type":"markdown","source":["## 2. Unsupervised Learning\n","- We take unlabeled data which means it is not categorized and corresponding outputs are also not given.\n","\n","- Now, this unlabeled input data is fed to the machine learning model in order to train it.\n","\n","- Firstly, it will interpret the raw data to find the hidden patterns from the data and then will apply suitable alogorithms such as k-means clusting, decission tree, etc..\n","\n","<img src=\"https://databasetown.com/wp-content/uploads/2023/05/Unsupervised-Learning-1536x1090.jpg\">\n","\n","## Types of Unsupervised Learning:\n","\n","- **Clustering or Cluster Analysis**\n","\n","- **Association Rule Learning**\n","\n","- **Dimensionality Reduction**\n","\n","- **Anomaly Detection**\n","\n","Check the unsupervised ipynb file for detailed explanation.\n"],"metadata":{"id":"RVawhkDuMO7N"}}]}