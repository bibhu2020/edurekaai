{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyONHI1ip/hxICH+1koZbOfJ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Unsupervised Learning\n","\n","Unsupervised learning is a type of machine learning where the model learns from unlabeled data ‚Äî meaning we do not provide output labels or categories.\n","\n","The goal is to discover hidden patterns, structures, or features in the data.\n","\n","- No labeled output (no Y values)\n","- The model finds:\n","  - Groups / Clusters\n","  - Structure\n","  - Anomalies\n","  - Reduced Dimensions\n","\n","**Use Cases**\n","- Customer Segmentation\n","- Document categorization\n","- Image grouping\n","\n","## Types of Unsupervised Learning:\n","\n","- **Clustering or Cluster Analysis**\n","\n","- **Association Rule Learning**\n","\n","- **Dimensionality Reduction**\n","\n","- **Anomaly Detection**\n","\n","**Note**: Classification vs Clustering - Classification has pre-defined classes, but Clustering has no known classes.\n","\n","https://www.naftaliharris.com/blog/visualizing-dbscan-clustering/\n","\n","### Cluster Analysis\n","\n","Cluster analysis is a type of unsupervised learning where we group similar data points together. The objective is to classify objects into clusters that are more similar to each other than to those in other clusters.\n","\n","#### Types of Clustering Algorithms\n","\n","1. **K-means Clustering**\n","   - Partitions data into **K** distinct clusters.\n","   - Each data point belongs to the cluster with the nearest mean.\n","\n","2. **Hierarchical Clustering**\n","   - Builds a hierarchy of clusters.\n","   - Can be agglomerative (bottom-up) or divisive (top-down).\n","\n","3. **DBSCAN (Density-Based Spatial Clustering of Applications with Noise)**\n","   - Groups together closely packed points.\n","   - Identifies outliers as noise.\n","\n","4. **Gaussian Mixture Models (GMM)**\n","   - Assumes data is generated from a mixture of several Gaussian distributions.\n","   - Soft clustering approach (probabilistic).\n","\n"],"metadata":{"id":"nxiLJJSGl0ow"}},{"cell_type":"markdown","source":["# 1. Clustering\n","\n"],"metadata":{"id":"D1kk1tds-9e8"}},{"cell_type":"markdown","source":["## 1.1. K-Means Clustering\n","K-means is an unsupervised machine learning algorithm used to group data into K distinct clusters, where each data point belongs to the cluster with the nearest centroid (mean of the cluster).\n","\n","- K = Number of cluster you define\n","- Centroid = Mean positions of all points in  a cluster\n","- Each point is assisgned to the cluster with the nearest centroid. (Hence, K-Means clustering is a distance-based clustering)\n","\n","![K-means Clustering Diagram](https://developers.google.com/static/machine-learning/clustering/images/clustering_example.png?hl=pt-br)\n","\n","K-means clustering is distance-based clustering. Steps below explains how the clustering takes place.\n","\n","### Steps in K-Means Algorithm\n","\n","**Step1**: Choose the number of clusters, K.\n","\n","**Step2**: Rendomly initialize K centroids. (Assume a mean for each cluster)\n","\n","**Step3**: Assign each point to the nearest centroid.\n","\n","**Step4**: Recalculate centroids by taking the mean of all assigned points.\n","\n","**Step5**: Repeat steps3 and 4 until centroids do not change (covergence).\n","\n","Animation Demonstating how K-means work >>\n","https://www.naftaliharris.com/blog/visualizing-k-means-clustering/\n","\n","### Evaluation Metrics for K-Means\n","\n","- **Intertia (WCSS)**: Sum of squared distances from points to their centroids.\n","\n","- **Silhouette Score**: How well a point fits in its cluster compated to others.\n","\n","- **Elbow Method**: Helps choose the best value of K by plotting WCSS vs K.\n","\n","#### 1.1.1 Intertia / WCSS\n","Inertia is a metric used to evaluate the quality of K-means clustering. It measures how tightly the data points are clustered around the centroids.\n","\n","- Low Inertia = points are close to their centroids ‚Üí good clustering\n","\n","- High Inertia = points are far from centroids ‚Üí poor clustering\n","\n","**Formulae**\n","\n","$$\n","\\text{Inertia} = \\sum_{i=1}^{k} \\sum_{x \\in C_i} \\|x - \\mu_i\\|^2\n","$$\n","\n","Where:\n","- $ k $: Number of clusters  \n","- $ C_i $: Set of points in cluster $ i $  \n","- $ \\mu_i $: Centroid of cluster $ i $  \n","- $ x $: Data point in cluster $ i $  \n","- $ \\|x - \\mu_i\\|^2 $: Squared Euclidean distance between point and centroid\n","\n","---\n","\n","**üí° Intuition**\n","\n","- **Low Inertia** = Points are close to centroids ‚Üí Good clustering\n","- **High Inertia** = Points are far from centroids ‚Üí Poor clustering\n","\n","‚ö†Ô∏è Note: Inertia **always decreases** when you increase the number of clusters (K), so it shouldn‚Äôt be used alone to determine the best K.\n","\n","##### 1.1.1. Elbow Method\n","The **Elbow Method** is a popular technique used in conjuction with WCSS to determine the optimal number of clusters $ k $ for K-Means clustering.\n","\n","- As the number of clusters $ k $ increases, the **Within-Cluster Sum of Squares (WCSS)** decreases.\n","- However, after a certain point, the improvement becomes marginal.\n","- The **\"elbow point\"** on the plot (k vs. WCSS) indicates the optimal number of clusters ‚Äî where the rate of decrease sharply changes.\n","\n","<img src=\"https://www.ibm.com/content/dam/connectedassets-adobe-cms/worldwide-content/creative-assets/s-migr/ul/g/88/64/k-means-clustering-graph.png\" alt=\"Elbow Method\" width=\"600\" height=\"300\">\n","\n","\n","---\n","\n","#### 1.1.2 Silhouette Score\n","The **Silhouette Score** is a metric used to evaluate how well the results of a clustering algorithm (like **K-Means**) are formed. It measures how similar each point is to its **own cluster** (cohesion) compared to other clusters (separation).\n","\n","---\n","\n","##### üß† Intuition\n","\n","- A **high Silhouette Score** means the point is **well matched to its own cluster** and **far from other clusters**.\n","- A **low or negative score** indicates the point may be **misclassified** or lies between clusters.\n","\n","---\n","\n","##### üìê Silhouette Score Formula\n","\n","For a single point **i**:\n","\n","- `a(i)`: average distance from **i** to all other points in the **same cluster**.\n","- `b(i)`: average distance from **i** to all points in the **nearest different cluster**.\n","\n","$$\n","s(i) = \\frac{b(i) - a(i)}{\\max\\{a(i), b(i)\\}}\n","$$\n","\n","- `s(i)` ranges between **-1 and +1**:\n","  - **+1**: ideal clustering\n","  - **0**: point is on the boundary\n","  - **-1**: likely misclassified\n","\n","<img src=\"https://miro.medium.com/v2/resize:fit:1400/1*Kq52h5BzHqPHQSwOAsLdfw.jpeg\" width=\"800\" height=\"500\">"],"metadata":{"id":"xHECRUlZlfih"}},{"cell_type":"markdown","source":["## 1.2. Hierarchical Clustering\n","Hierarchical clustering is an unsupervised learning algorithm used to group similar data points into nested clusters based on a hierarchy. Unlike K-means, it does not require specifying the number of clusters (K) in advance.\n","\n","2 Types of hierarchical clustering.\n","- Agglomerative hierarchical clustering\n","- Divisive hirerachical clustering (**not in use any more**)\n","\n","### Agglomerative Hierarchical Clustering\n","- Each datapoint is a cluster.\n","- Take distance of the datapoint from each other data points.\n","- Find out the closets cluster or datapoint, and then merge them to a cluster.\n","- In the end, this algorithm terminates when there is only a **single cluster left.**\n","- The results from the hierarchical clustering can be shown using **dendrogram**.\n","\n","<img src=\"https://www.researchgate.net/publication/378433323/figure/fig1/AS:11431281225406409@1708707906747/Hierarchical-clustering-dendrogram-The-figure-displays-the-hierarchical-clustering.png\">\n","\n","### Reading Dendogram\n","- Height of the blocks (vertical line) represent the **distance between the clusters.**\n","\n","- Distance between the observations/data points (horizental line) represents **dissimilarities.**. Observations are allocated to a cluster by drawing the horizental line.\n","\n","- Cut the dendogram in such a way that, it cuts the tallest vertical line. Clusters under the line is your starting cluster count.In the picture above, you could see there 4 clusters to start with. (orange, green, red, and purple).\n","**Note**: Cluster Count = No of times the Horizental line cuts the vertical = 4\n","\n","\n","\n","\n","\n","\n"],"metadata":{"id":"1ocWR0EK-1l_"}},{"cell_type":"markdown","source":["# 2. Association Rule Learning\n","Association Rule Learning is an unsupervised machine learning technique used to discover interesting relationships, patterns, or associations among variables in large datasets.\n","\n","It‚Äôs most commonly used in market basket analysis (e.g., \"If a customer buys bread & beer, they are likely to buy milk\"), marketing strategy and stock planning.\n","\n","Let‚Äôs say we have 5 transactions from a small store:\n","\n","| Transaction ID | Items Bought                    |\n","|----------------|----------------------------------|\n","| T1             | Milk, Bread                     |\n","| T2             | Milk, Diaper, Beer, Eggs        |\n","| T3             | Milk, Diaper, Beer, Bread       |\n","| T4             | Diaper, Beer                    |\n","| T5             | Milk, Bread, Diaper, Beer       |\n","\n","#### üî¢ Key Metrics\n","Example: **If a customer buys Diaper and Beer, they are likely to buy Milk**  \n","Rule: `A{Diaper, Beer} ‚áí B{Milk}`\n","\n","##### 1. **Support**\n","Indicates how frequently an item appears in the dataset.\n","\n","$$\n","\\text{Support}(B) = \\frac{\\text{Transactions containing } B}{\\text{Total transactions}}\n","$$\n","\n","- (B) = Milk appears in **T1, T2, T3, and T5** ‚Üí 4 transactions  \n","- Total transactions = 5\n","- Support = 4 / 5 = 0.8 (80%)\n","\n","**Support for B appearing in the basket is 0.8**\n","\n","##### 2. **Confidence**\n","Confidence measures how often the consequent (then-part) occurs when the antecedent (if-part) occurs. **How many times, B has appeared together in the basket when A was purchased.**\n","\n","$$\n","\\text{Confidence}(A \\Rightarrow B) = \\frac{\\text{Support}(A, B)}{\\text{Support}(A)}\n","$$\n","\n","- (A, B appearing together) = Diaper + Beer + Milk  appears in **T2, T3, T5** ‚Üí 3 transactions\n","- A = Diaper + Beer appears in **T2, T3, T4, T5** ‚Üí 4 transactions\n","- Total transactions = 5\n","- Support(A,B) = 3/5\n","- Support(A) = 4/5\n","- Confidence = Support(A,B) / Support(A)  = 3 / 4 = 0.75 (75%)\n","\n","##### 3. **Lift**\n","Lift measures how much more likely the consequent (then-part) is to occur with the antecedent (if-part) than by random chance.\n","\n","$$\n","\\text{Lift}(A \\Rightarrow B) = \\frac{\\text{Confidence}(A \\Rightarrow B)}{\\text{Support}(B)}\n","$$\n","\n","- Lift = 0.75 / 0.8 = 0.9375 (93.75%)\n","\n","> - **Lift > 1**: Positive association  \n","> - **Lift = 1**: No association  \n","> - **Lift < 1**: Negative association\n","\n","\n","---\n","\n","#### ‚öôÔ∏è Common Algorithms\n","\n","| Algorithm     | Description                                   |\n","|---------------|-----------------------------------------------|\n","| **Apriori**   | Iteratively finds frequent itemsets and prunes unlikely ones |\n","| **Eclat**     | Uses set intersections for fast frequency counting |\n","| **FP-Growth** | Uses a compact tree structure (FP-tree) to find frequent patterns without candidate generation |\n","\n","---\n","\n","#### üì¶ Use Cases\n","\n","| Domain          | Use Case                                        |\n","|-----------------|-------------------------------------------------|\n","| Retail          | Market basket analysis                          |\n","| E-commerce      | Product recommendation                          |\n","| Healthcare      | Detect co-occurring symptoms or conditions      |\n","| Web Usage       | Analyze browsing behavior or click patterns     |\n","\n","## 2.1 üîç Apriori Algorithm Explained\n","\n","The **Apriori algorithm** is a popular algorithm for **association rule mining** ‚Äî it finds patterns like:\n","\n","> \"If a customer buys **bread** and **butter**, they are also likely to buy **jam**.\"\n","\n","It‚Äôs commonly used in **market basket analysis**.\n","\n","---\n","\n","### üß† What is Apriori?\n","\n","Apriori identifies **frequent itemsets** (items often purchased together) and then generates **association rules** using:\n","\n","- **Support**: Frequency of occurrence in the dataset.\n","- **Confidence**: Strength of implication in a rule.\n","- **Lift**: How much more often items occur together than expected by chance.\n","\n","---\n","\n","### ‚öôÔ∏è How Apriori Works\n","\n","#### 1. **Set thresholds**\n","- **Support**: Minimum proportion of transactions containing an itemset.\n","- **Confidence**: Minimum reliability of a rule.\n","- **Lift** (optional): Measures the strength of a rule over random chance.\n","\n","#### 2. **Generate frequent itemsets**\n","- Count item frequencies.\n","- Eliminate infrequent items based on the support threshold.\n","- Use the **Apriori principle**: if a set is infrequent, all supersets are also infrequent.\n","\n","#### 3. **Generate association rules**\n","- For each frequent itemset, generate all valid rules and evaluate them using confidence and lift.\n","\n","---\n","\n","### üßæ Example Dataset\n","\n","| Transaction ID | Items                  |\n","|----------------|------------------------|\n","| 1              | Milk, Bread, Butter    |\n","| 2              | Bread, Butter          |\n","| 3              | Milk, Bread            |\n","| 4              | Milk, Butter           |\n","| 5              | Bread, Butter          |\n","\n","---\n","\n","### ‚úÖ Step-by-Step Example\n","\n","#### Step 1: Set thresholds\n","\n","- `min_support` = 0.6\n","- `min_confidence` = 0.7\n","\n","---\n","\n","#### Step 2: Frequent 1-itemsets\n","Eliminate the items that do not appear frequent in the basket (i.e the support level is less than `min_support`)\n","\n","| Item    | Support Count | Support |\n","|---------|----------------|---------|\n","| Milk    | 3              | 3/5 = 0.6 ‚úÖ |\n","| Bread   | 4              | 4/5 = 0.8 ‚úÖ |\n","| Butter  | 4              | 4/5 = 0.8 ‚úÖ |\n","\n","---\n","\n","#### Step 3: Frequent 2-itemsets\n","- Use the Frequent 1-itemsets from Step#2 to build the Item Pairs, and calculate it's support level.\n","\n","- Eliminate the item-pairs that have support level less than the `min_support`\n","\n","| Itemset         | Support Count | Support |\n","|------------------|----------------|---------|\n","| Milk, Bread      | 2              | 2/5 = 0.4 ‚ùå |\n","| Milk, Butter     | 2              | 2/5 = 0.4 ‚ùå |\n","| Bread, Butter    | 3              | 3/5 = 0.6 ‚úÖ |\n","\n","Only `Bread, Butter` is frequent.\n","\n","You could iterate the process to build 3-itemsets and calculate the support-level. And then eliminate the itemsets with support less than min_support.\n","\n","---\n","\n","#### Step 4: Generate Association Rules\n","\n","When you keep on repeating the steps above, you build the itemsets with support and confidence level. If the itemsets above the threshold set in Step#1, then they are your association rules.\n","\n","From `Bread, Butter`, we get:\n","\n","| Rule             | Support | Confidence |\n","|------------------|---------|------------|\n","| Bread ‚Üí Butter   | 0.6     | 3/4 = 0.75 ‚úÖ |\n","| Butter ‚Üí Bread   | 0.6     | 3/4 = 0.75 ‚úÖ |\n","\n","---\n","\n","### üìà Final Rules\n","Rule: Bread ‚Üí Butter (Support = 0.6, Confidence = 0.75)\n","Rule: Butter ‚Üí Bread (Support = 0.6, Confidence = 0.75)\n","\n","### üß™ Python Code Example\n","\n","```python\n","from mlxtend.frequent_patterns import apriori, association_rules\n","from mlxtend.preprocessing import TransactionEncoder\n","import pandas as pd\n","\n","# Sample transactions\n","transactions = [\n","    ['Milk', 'Bread', 'Butter'],\n","    ['Bread', 'Butter'],\n","    ['Milk', 'Bread'],\n","    ['Milk', 'Butter'],\n","    ['Bread', 'Butter']\n","]\n","\n","# Encode transactions\n","te = TransactionEncoder()\n","te_array = te.fit(transactions).transform(transactions)\n","df = pd.DataFrame(te_array, columns=te.columns_)\n","\n","# Apply Apriori\n","frequent_itemsets = apriori(df, min_support=0.6, use_colnames=True)\n","\n","# Generate rules\n","rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=0.7)\n","\n","# Display results\n","print(rules[['antecedents', 'consequents', 'support', 'confidence', 'lift']])\n","```"],"metadata":{"id":"wX0HpdNFGm89"}},{"cell_type":"markdown","source":["# 3. üåê Dimensionality Reduction\n","\n","**Dimensionality Reduction** is a technique used in data science and machine learning to reduce the number of input variables (or features) in a dataset while retaining as much relevant information as possible.\n","\n","**Dimensionality Reduction** is used as a pre-processing step.\n","\n","---\n","\n","## üß† Why it‚Äôs useful\n","\n","- **Removes noise**: Eliminates irrelevant or redundant features.\n","- **Speeds up training**: Fewer features = faster computations.\n","- **Improves performance**: Helps reduce overfitting and enhances model generalization.\n","- **Simplifies visualization**: Makes high-dimensional data viewable in 2D or 3D.\n","\n","---\n","\n","## üß∞ Common Techniques\n","\n","### üîç Feature Selection / Elimination Techniques\n","\n","| Technique                              | Category        | Description |\n","|---------------------------------------|-----------------|-------------|\n","| **Missing Value Ratio**               | Filter Method   | Removes features with a high percentage of missing values (e.g., >60%). These features may not contribute meaningfully to the model. |\n","| **Low Variance Filter**               | Filter Method   | Eliminates features with little to no variance across samples. Such features provide minimal or no predictive value. |\n","| **High Correlation Filter**           | Filter Method   | Removes one of two features that are highly correlated (e.g., correlation > 0.85), to avoid redundancy and multicollinearity. |\n","| **Random Forest**           | Filter Method   | Random Forest Feature Importance Technique can be used filter out important features. |\n","| **Forward Feature Selection**              | Wrapper Method   | Starts with no features and adds them one by one, selecting the feature that improves model performance the most at each step. |\n","| **Backward Feature Elimination**                   | Wrapper Method  | Starts with all features and removes the least useful one at each step based on model performance. |\n","\n","\n","---\n","\n","### üß™ Feature Engineering Techniques (Also called Extraction Techniques)\n","\n","| Technique                             | Description |\n","|--------------------------------------|-------------|\n","| **PCA (Principal Component Analysis)** | Projects data onto new axes (principal components) that capture the most variance. Reduces dimensionality while preserving as much information as possible. |\n","| **LDA (Linear Discriminant Analysis)** | Similar to PCA but supervised‚Äîit tries to maximize the separation between classes. Useful in classification tasks. |\n","| **Factor Analysis (FA)**             | A statistical method that models observed variables as combinations of a few underlying latent factors. Assumes these latent factors cause correlations between variables. |\n","| **Independent Component Analysis (ICA)** | Decomposes data into statistically independent components. Useful for signal separation and uncovering hidden factors. |\n","\n","\n","\n","---\n","\n","## üìâ Example\n","\n","Imagine you have a dataset with **1000 features** (columns) for each product, but only **10** of them are truly useful for predicting customer purchases.  \n","Dimensionality reduction helps you shrink that down ‚Äî say to **10 or 20** key features ‚Äî making the model both simpler and more accurate.\n","\n","---\n","\n","\n"],"metadata":{"id":"saG8xB_g52Pk"}},{"cell_type":"markdown","source":["## 3.1 Feature Engineering Techniques\n","\n","### 3.1.1 PCA (Principal Component Analysis)\n","**Principal Component Analysis (PCA)** is a widely used dimensionality reduction technique in machine learning and data analysis. It helps ***simplify datasets with many features by reducing the number of variables*** while preserving as much of the original variance (information) as possible.\n","\n","E.g. Using BMI instead of Weight and Height of person as a feature.\n","\n","PCA is a method to find the ***linear combination that features for as much variability as possible.***\n","\n","*   https://www.youtube.com/watch?v=dz8imS1vwIM\n","*   https://www.youtube.com/watch?v=S51bTyIwxFs\n","\n","\n","**A good visual explanation of PCA using Eigenvalue**.:\n","*   https://www.youtube.com/watch?v=FgakZw6K1QQ\n","*   https://www.youtube.com/watch?v=FD4DeN81ODY\n","\n","\n","\n"],"metadata":{"id":"Pdexop6rUMGy"}}]}