{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNqx01NTg3pKtw+5EeAcgNn"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# 🌳 Tree-Based Algorithms — A Quick Guide\n","**Tree-based algorithms** are a class of machine learning models that use decision trees as their core building blocks to make predictions. They are widely used for both classification and regression tasks.\n","\n","## 🔍 What Is a Decision Tree?\n","\n","A **decision tree** is a flowchart-like structure used for predictions, where:\n","- **Internal nodes or Decission nodes** represent decisions based on features (e.g., `age > 30?`)\n","- **Branches** represent the outcome of those decisions\n","- **Leaf nodes or Pure nodes** represent the final output (a class label or value)\n","\n","---\n","\n","## 🌳 Popular Tree-Based Algorithms\n","\n","| Algorithm           | Type                    | Description                                                                 |\n","|---------------------|-------------------------|-----------------------------------------------------------------------------|\n","| **Decision Tree (CART)** | Classification & Regression | Simple tree model. Can overfit if not pruned.                              |\n","| **Random Forest**        | Ensemble                 | Builds multiple trees and uses majority voting or averaging.               |\n","| **Gradient Boosting (GBM)** | Ensemble              | Sequentially builds trees to fix previous errors. Strong but slow.         |\n","| **XGBoost**              | Ensemble                 | Fast, regularized gradient boosting. Highly accurate and efficient.        |\n","| **LightGBM**             | Ensemble                 | Histogram-based boosting. Very fast and scalable.                          |\n","| **CatBoost**             | Ensemble                 | Boosting algorithm optimized for categorical data. Easy to use.            |\n","\n","---\n","\n","## ✅ Advantages\n","\n","- No need for feature scaling or normalization\n","- Handles both numeric and categorical data\n","- Robust to missing values\n","- Captures **non-linear** relationships well\n","- Often yields **high accuracy**\n","- Decision Trees are easy to interpret\n","\n","---\n","\n","## ⚠️ Disadvantages\n","\n","- Decision Trees are prone to **overfit** if not regularized\n","- Ensemble methods (e.g., GBM, XGBoost) can be **complex and slow**\n","- Less interpretable than linear models (especially with many trees)\n","\n","---\n","\n","## 🧠 When to Use Tree-Based Models?\n","\n","Use tree-based algorithms when:\n","- You have **complex or non-linear** data\n","- You're looking for **high predictive power**\n","- You have **mixed types of features** (e.g., numerical + categorical)\n","- You don't mind **longer training time** in exchange for better performance\n","\n","---\n","\n"],"metadata":{"id":"s5yysFD7Z2f-"}},{"cell_type":"markdown","source":["### Attribute Selection Measures (ASM)\n","\n","- While implementing a decission tree, the main issue arises that how to select the best attribute for the root node and for sub-nodes.\n","\n","- To solve such problems, there is a technique which is called as **Attribue Selection Measures (ASM)**.\n","\n","- ASM Techniques are:\n","  - Information Gain\n","  - Gini Index (is the default choice by decission tree)\n","  - Reduction in Variance (is an old approach. we will not discuss here)"],"metadata":{"id":"zCPqZB5ChcI7"}},{"cell_type":"markdown","source":["#### What is Information Gain?\n","\n","- Is the measurement of changes in **entropy** the segmentation of dataset based on an attribute.\n","\n","- It calculates how much information a feature provides us about a class.\n","\n","- Based on the information gain, we split the node and build the decission tree.\n","\n","#####💡 What is Entropy?\n","Entropy is a measure of **impurity**, **uncertainty**, or **disorder** in a dataset.  \n","It is used in decision tree algorithms (like **ID3**, **C4.5**, and **CART**) to determine the best attribute to split on.\n","\n","- If all elements are of the same class, entropy is **0** (pure).\n","- If elements are evenly distributed across classes, entropy is **high** (maximum uncertainty).\n","\n","$$\n","E(S) = -\\sum_{i=1}^{n} p_i \\log_2 p_i\n","$$\n","\n","#####📌 Explanation:\n","- $ E(S) $: Entropy of a dataset or set \\( S \\)\n","- $ p_i $: Probability of class \\( i \\) in the dataset\n","- $ n $: Total number of distinct classes\n","- The logarithm is base 2, so entropy is measured in **bits**\n","\n","\n","**Let's understand this with an example:**\n","\n","Predict whether a customer will **purchase a house** based on:\n","\n","- 💰 Salary\n","- ⏳ Years to Retirement\n","\n","---\n","\n","##### 📦 Dataset\n","\n","| Customer | Salary ($) | Years to Retirement | Will Purchase (Target) |\n","|----------|------------|----------------------|-------------------------|\n","| A        | 90,000     | 20                   | Yes (1)                 |\n","| B        | 85,000     | 18                   | Yes (1)                 |\n","| C        | 60,000     | 10                   | Yes (1)                 |\n","| D        | 70,000     | 25                   | No (0)                  |\n","| E        | 55,000     | 5                    | No (0)                  |\n","| F        | 50,000     | 6                    | No (0)                  |\n","\n","---\n","\n","##### 🔢 Step 1: Calculate Entropy at the Root Node\n","\n","There are:\n","- 3 buyers (1s)\n","- 3 non-buyers (0s)\n","\n","$$\n","H_{\\text{root}} = -p_1 \\log_2(p_1) - p_0 \\log_2(p_0)\n","= -0.5 \\log_2(0.5) - 0.5 \\log_2(0.5) = 1.0\n","$$\n","\n","> The root node entropy is **1.0**, indicating maximum impurity.\n","\n","---\n","\n","##### 🧪 Step 2: Try Splitting on `Salary > 75,000`\n","\n","###### Split:\n","\n","- Left: A, B → [1, 1]\n","- Right: C, D, E, F → [1, 0, 0, 0]\n","\n","##### Entropy Calculation\n","\n","###### Left node:\n","$$\n","H = -1 \\log_2(1) = 0.0 \\quad \\text{(Pure Node)}\n","$$\n","\n","###### Right node:\n","- 1 buyer, 3 non-buyers → $ p_1 = 0.25, p_0 = 0.75 $\n","\n","$$\n","H = -\\left(0.25 \\log_2 0.25 + 0.75 \\log_2 0.75 \\right)\n","= - (0.25 \\cdot 2 + 0.75 \\cdot 0.415)\n","= 0.811\n","$$\n","\n","##### Weighted Entropy After Split\n","\n","$$\n","H_{\\text{split}} = \\frac{2}{6} \\cdot 0.0 + \\frac{4}{6} \\cdot 0.811 = 0.541\n","$$\n","\n","##### Information Gain\n","\n","$$\n","IG = H_{\\text{root}} - H_{\\text{split}} = 1.0 - 0.541 = \\textbf{0.459}\n","$$\n","\n","---\n","\n","##### 🧪 Step 3: Try Splitting on `Years to Retirement < 15`\n","\n","###### Split:\n","\n","- Left: C, E, F → [1, 0, 0]\n","- Right: A, B, D → [1, 1, 0]\n","\n","###### Left node (1 buyer, 2 non-buyers):\n","\n","$$\n","H = -\\left(\\frac{1}{3} \\log_2 \\frac{1}{3} + \\frac{2}{3} \\log_2 \\frac{2}{3}\\right)\n","= - (0.528 + 0.389) = 0.918\n","$$\n","\n","###### Right node (2 buyers, 1 non-buyer):\n","\n","Same proportions → $ H = 0.918 $\n","\n","##### Weighted Entropy After Split\n","\n","$$\n","H_{\\text{split}} = \\frac{3}{6} \\cdot 0.918 + \\frac{3}{6} \\cdot 0.918 = 0.918\n","$$\n","\n","##### Information Gain\n","\n","$$\n","IG = 1.0 - 0.918 = \\textbf{0.082}\n","$$\n","\n","---\n","\n","##### 📊 Comparison Table\n","\n","| Feature               | Best Threshold     | Entropy After | Information Gain |\n","|-----------------------|--------------------|----------------|------------------|\n","| **Salary**            | > 75,000           | 0.541          | **0.459** ✅     |\n","| Years to Retirement   | < 15               | 0.918          | 0.082            |\n","\n","> ✅ **Salary > 75,000** gives the **highest information gain**, so it is chosen as the first split.\n","\n","Further splitting can be done on [C, D, E, F] using `Years to Retirement`.\n","\n","---\n","\n","##### 🌳 Decision Tree Structure (Using Entropy)\n","\n","                            [ROOT]\n","                      Salary > 75,000?\n","                      /              \\\n","                 Yes (A, B)          No (C, D, E, F)\n","               [Leaf: Buy=2]         |\n","                               Years to Retirement < 8?\n","                               /                 \\\n","                         Yes (E, F)          No (C, D)\n","                     [Leaf: No Buy=2]     |\n","                                   Salary > 65,000?\n","                                   /            \\\n","                           Yes (D)            No (C)\n","                     [Leaf: No Buy=1]     [Leaf: Buy=1]\n","\n","\n","---\n","\n","##### 🧠 Conclusion\n","\n","- The decision tree chooses the split that results in the **highest information gain**.\n","- **Salary > 75,000** is the best feature to split on, both in Gini and Entropy criteria.\n","- Entropy quantifies impurity in bits. Lower entropy → more pure node.\n","- **Information Gain** tells us how much entropy was reduced after a split.\n","\n"],"metadata":{"id":"9E3I-9HuiZpG"}},{"cell_type":"markdown","source":["###💡 What is Gini Index?\n","The Gini Index measures the **impurity** of a dataset — like entropy, it is used in decision trees (especially in **CART** - Classification And Regression Tree) to decide the best split.\n","\n","- **Gini = 0** → Pure node or Leaf node (all elements belong to one class)\n","- **Higher Gini** → More mixed or impure\n","\n","$$\n","Gini(S) = 1 - \\sum_{i=1}^{n} p_i^2\n","$$\n"," - An attribute / feature with the low Gini index should be preferred for branching over one with higher Gini Index.\n"," - It only creates binary splits.\n","\n","#####📌 Explanation:\n","- $ Gini(S) $: Gini impurity of dataset or node \\( S \\)\n","- $ p_i $: Probability of class \\( i \\)\n","- $ n $: Number of classes\n","\n","#####✅ Example:\n","Predict whether a customer will **purchase a house** based on:\n","- 💰 **Salary**\n","- ⏳ **Years to Retirement**\n","\n","---\n","\n","##### 🧾 Dataset\n","\n","| Customer | Salary ($) | Years to Retirement | Will Purchase (Target) |\n","|----------|------------|----------------------|-------------------------|\n","| A        | 90,000     | 20                   | Yes (1)                 |\n","| B        | 85,000     | 18                   | Yes (1)                 |\n","| C        | 60,000     | 10                   | Yes (1)                 |\n","| D        | 70,000     | 25                   | No (0)                  |\n","| E        | 55,000     | 5                    | No (0)                  |\n","| F        | 50,000     | 6                    | No (0)                  |\n","\n","---\n","\n","##### 🔢 Step 1: Calculate Gini at Root Node\n","\n","We have:\n","- Buyers (1s): 3\n","- Non-buyers (0s): 3\n","\n","$$\n","\\text{Gini}_{\\text{root}} = 1 - (0.5)^2 - (0.5)^2 = 0.5\n","$$ ➡️ This is **maximum impurity** — completely mixed classes at the root node.\n","\n","---\n","\n","##### 🧪 Step 2: Try Splitting on `Salary > 75,000`\n","\n","| Node | Customers       | Buyers | Non-buyers | Gini |\n","|------|------------------|--------|------------|------|\n","| Left (Yes)  | A, B              | 2      | 0          | 0.0 ✅ (Pure) |\n","| Right (No)  | C, D, E, F        | 1      | 3          | $$\n","G = 1 - \\left(\\frac{1}{4}\\right)^2 - \\left(\\frac{3}{4}\\right)^2 = 0.375\n","$$\n","\n","📉 **Weighted Gini After Split**:\n","$$\n","G_{\\text{split}} = \\left(\\frac{2}{6} \\times 0.0\\right) + \\left(\\frac{4}{6} \\times 0.375\\right) = 0.25\n","$$\n","\n","➡️ **Gini Reduction**:  \n","$$\n","0.5 \\rightarrow 0.25 = \\textbf{0.25 reduction}\n","$$\n","\n","---\n","\n","##### 🧪 Step 3: Try Splitting on `Years to Retirement < 15`\n","\n","| Node | Customers       | Buyers | Non-buyers | Gini |\n","|------|------------------|--------|------------|------|\n","| Left (Yes) | C, E, F         | 1      | 2          | $$\n","G = 1 - \\left(\\frac{1}{3}\\right)^2 - \\left(\\frac{2}{3}\\right)^2 = 0.444\n","$$\n","| Right (No) | A, B, D         | 2      | 1          | $$\n","G = 1 - \\left(\\frac{2}{3}\\right)^2 - \\left(\\frac{1}{3}\\right)^2 = 0.444\n","$$\n","\n","📉 **Weighted Gini After Split**:\n","$$\n","G_{\\text{split}} = (0.444 \\times 0.5) + (0.444 \\times 0.5) = 0.444\n","$$\n","\n","➡️ **Gini Reduction**:  \n","$$\n","0.5 \\rightarrow 0.444 = \\textbf{0.056 reduction}\n","$$\n","\n","---\n","\n","##### ✅ Step 4: Compare the Features\n","\n","| Feature               | Best Threshold | Gini After | Gini Reduction |\n","|------------------------|----------------|-------------|----------------|\n","| **Salary**             | > 75,000       | 0.25        | **0.25** ✅ |\n","| **Years to Retirement**| < 15           | 0.444       | 0.056          |\n","\n","📌 **Conclusion**:  \n","Since `Salary > 75,000` gives the **largest impurity reduction**, it is chosen for the first split.\n","\n","---\n","\n","##### 🌳 Final Tree After First Split\n","                          [ROOT]\n","                 Salary > 75,000?\n","                    /         \\\n","                 Yes           No\n","            [Leaf: Buy=2]   [C, D, E, F]\n","                              |\n","                   Years to Retirement < 8?\n","                          /       \\\n","                       Yes         No\n","                   [E, F]         [C, D]\n","                 [Leaf: No=2]  Years < 15?\n","                                   /    \\\n","                               Yes      No\n","                             [C]       [D]\n","                        [Leaf: Buy=1] [Leaf: No=1]\n","\n","| Leaf Node                        | Customers     | Class Prediction |\n","|----------------------------------|---------------|------------------|\n","| Salary > 75,000 (Yes)            | A, B          | ✅ Buy (1)       |\n","| Salary ≤ 75,000 & YTR < 8 (Yes)  | E, F          | ❌ No Buy (0)    |\n","| Salary ≤ 75,000 & YTR ≥ 8 & <15  | C             | ✅ Buy (1)       |\n","| Salary ≤ 75,000 & YTR ≥ 15       | D             | ❌ No Buy (0)    |\n","\n","---\n","\n","##### ✅ Summary\n","\n","- **Salary > 75,000** was the most informative first split (Gini ↓ 0.25).\n","- Further splits used **Years to Retirement** to purify the remaining mixed nodes.\n","- Every final leaf is **pure** (all samples belong to the same class).\n","\n","\n"],"metadata":{"id":"te14X47usOYJ"}}]}