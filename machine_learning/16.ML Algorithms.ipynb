{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNYgZNQWidebe1sRpMV3Sia"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## Popular ML Algorithms\n","\n","### Regression (output is continious. e.g. housing price prediction)\n","\n","**1. Linear Regression**\n","\n","This algorithm is used to find the best-fit line that has minimum distance from the data points. It operates on the assumption that the output is linearly related to the input features.\n","\n","**_Pros_**\n","- it is simple mathmatical model and easily explainable.\n","\n","**_Cons_**\n","- Not all data in the realworld is linear in nature.\n","\n","**2. Decission Tree**\n","\n","It plots the data in a decision tree. In the tree building process, it tries to ensure entropy drop and information gain.\n","\n","**_Pros_**\n","- Tree structure is easily explanable.\n","- No feature scaling is needed.\n","- Handles both numerical and categorical data.\n","- Handles non-linear data\n","- Handles missing values\n","\n","**_Cons_**\n","- Can easily memorize training data if not pruned or regularized and that could lead to overfitting.\n","- Sensitive to noise — one outlier can change splits drastically.\n","\n","**3. Random Forest**\n","\n","- Bagging (Bootstrap Aggregation) of many Decision Trees.\n","- Each tree trained on a random sample of data + random subset of features.\n","- All trees are trained in parallel.\n","- Final prediction = majority vote (classification) or average (regression) from the trees.\n","\n","\n","**_Pros_**\n","- Reduces overfitting compared to a single tree.\n","- Robust to noise, handles missing values.\n","- General-purpose strong model for tabular data.\n","\n","**_Cons_**\n","- Less interpretable.\n","- Slower training & prediction than single tree.\n","\n","**4. Ada Boost (or Adaptive Boost)**\n","It is the type of Ensemble learning where the trees are trained in sequence. Sequentially trains weak learners (usually shallow trees).\n","\n","- Model1 is trained with equal weights to all features\n","- Error Residual is calculated from Model1 prediction.\n","- Feature weights are adjusted based on Error Residual, and then sent to the Model2.\n","- Error Residual is calculated from Model2 prediction.\n","- Feature weights are adjusted based on Error Residual, and then sent to the Model3.\n","- And so on...\n","\n","**_Pros_**\n","- This gives more accurate result because it adapts to the error and adjusts the weight.\n","- Good at reducing bias.\n","- Works well on simple, weak learners.\n","- When you want better performance than a single decision tree, but dataset isn’t huge.\n","\n","**_Cons_**\n","- Sensitive to noisy data and outliers.\n","- Slower than Random Forest.\n","\n","\n","**5. Gradient Boost**\n","Sequential boosting like AdaBoost, but instead of reweighting data, it fits the next tree to the residual errors of the previous model (gradient descent approach).\n","\n","**_Pros_**\n","- Usually more accurate than AdaBoost.\n","- Handles complex non-linear relationships well.\n","\n","**_Cons_**\n","- Slower to train than Random Forest.\n","- Hyperparameters (learning rate, n_estimators, depth) need careful tuning.\n","\n","**6. XGB**\n","Optimized version of Gradient Boosting with:\n","- Regularization (to reduce overfitting).\n","- Parallelized training.\n","- Efficient handling of missing data.\n","\n","**_Pros_**\n","- Faster and more accurate than vanilla Gradient Boosting.\n","- Industry standard for Kaggle/tabular ML competitions.\n","\n","**_Cons_**\n","- More complex to tune.\n","- Can overfit if not regularized well.\n","\n","### Classification (output is discreat. E.g. predicting potential buyer yes/no)\n","\n","**7. Logistic Regression**\n","\n","**8. SVM**\n","\n","**9. Linear Regression**\n","\n","### Unsupervised (no labeled-output available.)\n","\n","**10. K-Means**\n","\n","**11. Calliberative Filtering**\n"],"metadata":{"id":"AJBinZY8k7ql"}}]}